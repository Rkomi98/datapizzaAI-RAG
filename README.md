# Chatbot RAG FAQ Datapizza-AI

Un chatbot RAG intelligente che risponde alle domande frequenti su Datapizza-AI, costruito utilizzando il framework datapizza-ai.

## ğŸ¯ Descrizione

Questo progetto implementa un sistema RAG (Retrieval-Augmented Generation) completo che:
- **Analizza** le FAQ su Datapizza-AI contenute in file markdown
- **Risponde** alle domande degli utenti basandosi esclusivamente sulle informazioni nelle FAQ
- **Restituisce** un messaggio specifico quando non trova informazioni rilevanti: "Non sono ancora state fatte domande a riguardo."

## âœ¨ Caratteristiche

- ğŸŒ **Interfaccia web moderna** con Streamlit - UI pulita e intuitiva
- ğŸ” **Retrieval semantico** con embeddings Google (default: gemini-embedding-001)
- ğŸ§  **Query rewriting** per migliorare il retrieval
- ğŸ’¾ **Vector store** Qdrant per memorizzazione efficiente
- ğŸ¤– **Generazione risposte** con Google Gemini 2.5 Flash
- ğŸ’¬ **Memory attiva** - mantiene il contesto della conversazione
- ğŸ”„ **Doppia interfaccia** - Web e terminale
- ğŸ“ **Pipeline modulare** facilmente estensibile
- ğŸ¨ **Design minimal** e user-friendly

## ğŸ—ï¸ Architettura

### Pipeline di Ingestion
```
File Markdown â†’ TextParser â†’ NodeSplitter â†’ ChunkEmbedder â†’ Qdrant VectorStore
```

### Pipeline di Retrieval (DagPipeline)
```
Query Utente â†’ ToolRewriter â†’ Embedder â†’ VectorStore Retrieval â†’ Prompt Template â†’ Generator (Gemini 2.5 Flash) + Memory
```

### Vista d'insieme del RAG (FAQ + docs ufficiali)
```mermaid
graph TD
  subgraph "Ingestion FAQ"
    A1["Markdown FAQ<br/>(datapizza_faq.md, FAQ_Video.md, Scripts/*.md)"] --> P1[TextParser]
    P1 --> S1["Node/Recursive Splitter"]
    S1 --> E1["ChunkEmbedder<br/>Google Gemini"]
    E1 --> Q1["Qdrant collection:<br/>datapizzai_faq"]
  end

  subgraph "Ingestion Docs (MCP)"
    B1["GitHub repo<br/>(datapizza-ai/docs)"] --> P2[TextParser]
    P2 --> S2[RecursiveSplitter]
    S2 --> E2["ChunkEmbedder<br/>OpenAI text-embedding-3-small"]
    E2 --> Q2["Qdrant collection:<br/>datapizza_official_docs"]
  end

  subgraph "Retrieval & Reasoning"
    U["Domanda utente"] --> RW["ToolRewriter<br/>(Gemini)"]
    RW --> GE["Google Embedder"]
    GE --> VR1["Qdrant search<br/>datapizzai_faq"]

    U -.->|MCP| EMB["OpenAI Embedding"]
    EMB --> VR2["Qdrant search<br/>datapizza_official_docs"]

    VR1 --> CTX["Context builder"]
    VR2 --> CTX
    CTX --> PR["Prompt Template"]
    PR --> LLM["Gemini 2.5 Flash"]
    LLM --> OUT["Risposta finale"]
    MEM[(Memory)] <--> LLM
  end

  APP["Streamlit app.py"] --> OUT
  TEST["test_mcp_retriever.py"] --> VR2
```

## ğŸš€ Quick Start

### Opzione A: Interfaccia Web (Consigliata) ğŸŒ

```bash
# 1. Attiva l'environment
source rag/bin/activate

# 2. Configura .env con la tua API key OpenAI

# 3. Avvia Qdrant (terminale separato)
docker run -p 6333:6333 -p 6334:6334 qdrant/qdrant

# 4. Avvia il frontend web
./run_web.sh
# oppure: streamlit run app.py
```

L'interfaccia web si aprirÃ  automaticamente nel browser! ğŸ‰

### Opzione B: Terminale ğŸ’»

```bash
# 1. Attiva l'environment
source rag/bin/activate

# 2. Configura .env con la tua API key Google (Gemini)

# 3. Avvia Qdrant (terminale separato)
docker run -p 6333:6333 -p 6334:6334 qdrant/qdrant

# 4. Avvia il chatbot da terminale
python chatbot_faq.py
# oppure: ./run_chatbot.sh
```

### Setup Completo (Prima Volta)

```bash
# 1. Attiva l'environment virtuale
source rag/bin/activate

# 2. Installa le dipendenze
pip install -r requirements.txt

# 3. Configura le variabili d'ambiente
cp .env.example .env
# Modifica .env e inserisci la tua API key

# 4. Avvia Qdrant
docker run -p 6333:6333 -p 6334:6334 qdrant/qdrant

# 5. Verifica il setup
python test_setup.py

# 6. Esegui l'ingestion delle FAQ
python ingest_faq.py

# 7. Avvia il chatbot (web o terminale)
./run_web.sh          # Interfaccia web
# oppure
python chatbot_faq.py # Terminale
```

## ğŸ’¡ Esempi di Utilizzo

```
ğŸ‘¤ Tu: Cosa differenzia Datapizza-AI da Langchain?

ğŸ¤– Bot: La differenza principale con altri framework Ã¨ il diverso livello
di astrazione dei moduli. Langchain usa astrazioni troppo elevate che non
permettono di uscire facilmente dai binari imposti...

ğŸ‘¤ Tu: Supporta modelli open-source?

ğŸ¤– Bot: SÃ¬, il framework supporta anche modelli Llama. Nella documentazione
su docs.datapizza.ai puoi trovare le istruzioni per eseguire un client
Llama o un server Llama in locale...

ğŸ‘¤ Tu: Che cos'Ã¨ la fotosintesi clorofilliana?

ğŸ¤– Bot: Non sono ancora state fatte domande a riguardo.
```

## ğŸ“ Struttura del Progetto

```
datapizzaAI-RAG/
â”œâ”€â”€ app.py                    # ğŸŒ Frontend web (Streamlit) - NUOVO!
â”œâ”€â”€ chatbot_faq.py            # ğŸ¤– Chatbot RAG core
â”œâ”€â”€ ingest_faq.py             # ğŸ“¥ Script per ingestion FAQ
â”œâ”€â”€ datapizza_faq.md          # ğŸ“„ FAQ generali su Datapizza-AI
â”œâ”€â”€ FAQ_Video.md              # ğŸ¥ FAQ estratte da video tutorial
â”œâ”€â”€ test_setup.py             # âœ… Script per verificare il setup
â”œâ”€â”€ test_chatbot.py           # ğŸ§ª Test automatici
â”œâ”€â”€ check_qdrant.py           # ğŸ” Verifica contenuto Qdrant
â”œâ”€â”€ run_web.sh                # ğŸŒ Avvio rapido frontend web - NUOVO!
â”œâ”€â”€ run_chatbot.sh            # ğŸ’» Avvio rapido terminale
â”œâ”€â”€ requirements.txt          # ğŸ“¦ Dipendenze Python
â”œâ”€â”€ .env                      # ğŸ” Configurazione (da creare)
â”œâ”€â”€ .env.example              # ğŸ“‹ Template configurazione
â”œâ”€â”€ .gitignore                # ğŸš« Esclusioni Git
â”œâ”€â”€ README.md                 # ğŸ“– Questo file
â”œâ”€â”€ START_HERE.md             # ğŸš€ Guida avvio rapido
â”œâ”€â”€ USAGE_GUIDE.md            # ğŸ“š Guida utente dettagliata
â”œâ”€â”€ WEB_FEATURES.md           # ğŸŒ Caratteristiche frontend
â”œâ”€â”€ INTERFACE_PREVIEW.md      # ğŸ¨ Anteprima interfaccia
â””â”€â”€ setup_instructions.md     # ğŸ› ï¸ Istruzioni setup complete
```

## ğŸ› ï¸ Tecnologie Utilizzate

- **[Datapizza-AI](https://docs.datapizza.ai/)** - Framework GenAI modulare
- **[Streamlit](https://streamlit.io/)** - Framework per interfaccia web interattiva
- **[Google Gemini](https://ai.google.dev/)** - Embeddings (gemini-embedding-001 di default) e LLM (Gemini 2.5 Flash)
- **[Qdrant](https://qdrant.tech/)** - Vector database per similarity search
- **Python 3.13+** - Linguaggio di programmazione

## ğŸ“– Componenti Principali

### ingest_faq.py
Script che implementa la **IngestionPipeline** per:
- Leggere i file markdown delle FAQ
- Dividere il testo in chunks semantici
- Generare embeddings con Google (gemini-embedding-001, 3072 dimensioni di default â€“ personalizzabili impostando `FAQ_EMBEDDING_MODEL` o `FAQ_EMBEDDING_DIM`)
- Includere automaticamente i copioni in inglese della cartella `Scripts/`, marcandoli con metadati `language="en"` e `type="scripts"`
- Salvare nel vector store Qdrant

### chatbot_faq.py
Implementa il chatbot usando **DagPipeline** con:
- Query rewriting per migliorare il retrieval
- Embedding della query
- Retrieval semantico dei chunks rilevanti
- Generazione risposta contestualizzata con Gemini 2.5 Flash
- Memory per mantenere il contesto della conversazione
- Fallback message quando non trova informazioni

## ğŸ”§ Configurazione Avanzata

### Parametri del Chatbot

Nel file `chatbot_faq.py` puoi modificare:

```python
# Numero di chunks da recuperare
k = 10

# Soglia minima di similarity score
score_threshold = 0.5

# Dimensione massima dei chunks
max_char = 2000
```

### Modelli Alternativi

Puoi cambiare i modelli Google con altre varianti:

```python
# Per embeddings - altre opzioni Google
embedder = GoogleEmbedder(
    model_name="gemini-embedding-001"  # Default attuale (imposta FAQ_EMBEDDING_MODEL per cambiarlo)
)

# Per generazione - altri modelli Gemini
client = GoogleClient(
    model="gemini-2.5-flash"  # Gemini 2.5 Flash (attuale)
)

# Oppure altri provider (OpenAI, Anthropic, Mistral, etc.)
from datapizza.clients.openai import OpenAIClient
client = OpenAIClient(model="gpt-4o")
```

## ğŸ› Troubleshooting

### "GOOGLE_API_KEY non trovata"
â†’ Crea il file `.env` e inserisci la tua API key Google:
```bash
GOOGLE_API_KEY=your-google-api-key-here
```
â†’ Ottieni una chiave API da: https://ai.google.dev/

### "Connection refused" (Qdrant)
â†’ Verifica che Qdrant sia in esecuzione: `docker ps | grep qdrant`

### "Collection not found"
â†’ Esegui prima l'ingestion: `python ingest_faq.py`
â†’ NOTA: Se hai giÃ  una collection con dimensioni 1536 (OpenAI), devi ricrearla!

### Risposte sempre "Non sono ancora state fatte domande..."
â†’ Verifica che l'ingestion sia andata a buon fine
â†’ Prova ad abbassare la `score_threshold`
â†’ Verifica che gli embeddings siano stati creati correttamente

## ğŸ“š Risorse

- [Documentazione Datapizza-AI](https://docs.datapizza.ai/)
- [Guida RAG](https://docs.datapizza.ai/0.0.2/Guides/RAG/rag/)
- [Google AI Studio](https://ai.google.dev/)
- [Qdrant Documentation](https://qdrant.tech/documentation/)
- [Gemini API Documentation](https://ai.google.dev/docs)

## ğŸ¤ Contribuire

Questo Ã¨ un progetto di esempio. Sentiti libero di:
- Aggiungere nuove FAQ
- Migliorare i prompt
- Sperimentare con diversi modelli
- Estendere le funzionalitÃ 

## ğŸ“ Licenza

Progetto di esempio per dimostrare le capacitÃ  di Datapizza-AI.

## ğŸ‘¥ Autore

Progetto creato come esempio di utilizzo del framework [Datapizza-AI](https://github.com/datapizza-labs/datapizza-ai)
